{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luHHuJgsVUk5",
        "outputId": "903c747d-a7a6-4305-cfb6-d785b363a803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the dataset\n",
        "data = pd.read_csv('sample_dataset.csv')\n",
        "text = ' '.join(data['text_column'])\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Stop-word Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "print(\"Tokens:\", tokens[:10])\n",
        "print(\"Filtered Tokens:\", filtered_tokens[:10])\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sYtx-h7VV7W",
        "outputId": "38e40516-b093-425a-a23b-fd5ecdae3bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of']\n",
            "Filtered Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', 'focuses']\n",
            "Lemmatized Tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'field', 'artificial', 'intelligence', 'focus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Frequency Analysis\n",
        "word_freq = Counter(lemmatized_tokens)\n",
        "print(\"Word Frequency:\", word_freq.most_common(10))\n",
        "\n",
        "# TF-IDF\n",
        "corpus = data['text_column'].tolist()\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "print(\"TF-IDF Scores:\", tfidf_matrix.toarray()[:10])\n",
        "\n",
        "# Sentiment Analysis\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_scores = sia.polarity_scores(text)\n",
        "print(\"Sentiment Scores:\", sentiment_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwPSAZOaWXXg",
        "outputId": "53a8448d-9135-49fb-f98d-7941b8717a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Frequency: [('NLP', 5), ('.', 5), (',', 5), ('human', 3), ('language', 2), ('computer', 2), ('report', 2), ('Natural', 1), ('processing', 1), ('(', 1)]\n",
            "TF-IDF Scores: [[0.         0.         0.         0.12599134 0.         0.26440698\n",
            "  0.         0.21332189 0.         0.         0.         0.\n",
            "  0.21332189 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26440698 0.26440698 0.\n",
            "  0.         0.         0.         0.         0.26440698 0.\n",
            "  0.         0.26440698 0.26440698 0.         0.17707644 0.26440698\n",
            "  0.         0.         0.         0.26440698 0.12599134 0.21332189\n",
            "  0.26440698 0.         0.         0.26440698 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26440698 0.17707644 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.23580313 0.         0.\n",
            "  0.         0.1996247  0.24742967 0.         0.         0.24742967\n",
            "  0.1996247  0.         0.         0.         0.24742967 0.24742967\n",
            "  0.         0.24742967 0.         0.         0.         0.\n",
            "  0.24742967 0.         0.         0.49485934 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.24742967 0.         0.         0.         0.11790156 0.\n",
            "  0.         0.         0.24742967 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16570653 0.\n",
            "  0.16570653 0.         0.24742967 0.         0.        ]\n",
            " [0.         0.         0.         0.12578376 0.         0.\n",
            "  0.26397133 0.         0.         0.         0.26397133 0.\n",
            "  0.         0.26397133 0.         0.         0.         0.\n",
            "  0.26397133 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.26397133 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.26397133 0.         0.12578376 0.\n",
            "  0.         0.         0.         0.         0.26397133 0.\n",
            "  0.26397133 0.26397133 0.26397133 0.         0.         0.26397133\n",
            "  0.         0.         0.         0.         0.17678468 0.26397133\n",
            "  0.17678468 0.         0.         0.26397133 0.        ]\n",
            " [0.         0.         0.         0.13811675 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.28985351 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.28985351 0.         0.         0.28985351 0.19411827 0.\n",
            "  0.         0.28985351 0.         0.         0.13811675 0.\n",
            "  0.         0.28985351 0.         0.         0.         0.28985351\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.28985351 0.28985351 0.28985351 0.         0.         0.\n",
            "  0.         0.28985351 0.         0.         0.28985351]\n",
            " [0.24252094 0.24252094 0.24252094 0.11556253 0.24252094 0.\n",
            "  0.         0.         0.         0.24252094 0.         0.\n",
            "  0.         0.         0.         0.24252094 0.         0.\n",
            "  0.         0.         0.24252094 0.         0.         0.24252094\n",
            "  0.         0.24252094 0.         0.         0.         0.24252094\n",
            "  0.         0.         0.         0.         0.1624191  0.\n",
            "  0.         0.         0.         0.         0.11556253 0.19566437\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24252094 0.48504189 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.1624191  0.         0.         0.         0.        ]]\n",
            "Sentiment Scores: {'neg': 0.0, 'neu': 0.894, 'pos': 0.106, 'compound': 0.8176}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "uAECj5ylf3Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Summarization\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)[0]['summary_text']\n",
        "print(\"Summary:\", summary)\n",
        "\n",
        "# Template-based Report Generation\n",
        "report_template = f\"\"\"\n",
        "Automatic Report\n",
        "Summary:\n",
        "{summary}\n",
        "Word Frequency:\n",
        "{word_freq.most_common(10)}\n",
        "TF-IDF Scores:\n",
        "{tfidf_matrix.toarray()[:10]}\n",
        "Sentiment Scores:\n",
        "{sentiment_scores}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Generated Report:\")\n",
        "print(report_template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx0-ljV8ae-e",
        "outputId": "7b8e035f-acde-43f3-c987-c2a7e6bd1788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:  Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans . NLP enables computers to understand and process human languages . Automatic report generation using NLP can significantly reduce the time and effort\n",
            "Generated Report:\n",
            "\n",
            "Automatic Report\n",
            "Summary:\n",
            " Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans . NLP enables computers to understand and process human languages . Automatic report generation using NLP can significantly reduce the time and effort\n",
            "Word Frequency:\n",
            "[('NLP', 5), ('.', 5), (',', 5), ('human', 3), ('language', 2), ('computer', 2), ('report', 2), ('Natural', 1), ('processing', 1), ('(', 1)]\n",
            "TF-IDF Scores:\n",
            "[[0.         0.         0.         0.12599134 0.         0.26440698\n",
            "  0.         0.21332189 0.         0.         0.         0.\n",
            "  0.21332189 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26440698 0.26440698 0.\n",
            "  0.         0.         0.         0.         0.26440698 0.\n",
            "  0.         0.26440698 0.26440698 0.         0.17707644 0.26440698\n",
            "  0.         0.         0.         0.26440698 0.12599134 0.21332189\n",
            "  0.26440698 0.         0.         0.26440698 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.26440698 0.17707644 0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.23580313 0.         0.\n",
            "  0.         0.1996247  0.24742967 0.         0.         0.24742967\n",
            "  0.1996247  0.         0.         0.         0.24742967 0.24742967\n",
            "  0.         0.24742967 0.         0.         0.         0.\n",
            "  0.24742967 0.         0.         0.49485934 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.24742967 0.         0.         0.         0.11790156 0.\n",
            "  0.         0.         0.24742967 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.16570653 0.\n",
            "  0.16570653 0.         0.24742967 0.         0.        ]\n",
            " [0.         0.         0.         0.12578376 0.         0.\n",
            "  0.26397133 0.         0.         0.         0.26397133 0.\n",
            "  0.         0.26397133 0.         0.         0.         0.\n",
            "  0.26397133 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.26397133 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.26397133 0.         0.12578376 0.\n",
            "  0.         0.         0.         0.         0.26397133 0.\n",
            "  0.26397133 0.26397133 0.26397133 0.         0.         0.26397133\n",
            "  0.         0.         0.         0.         0.17678468 0.26397133\n",
            "  0.17678468 0.         0.         0.26397133 0.        ]\n",
            " [0.         0.         0.         0.13811675 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.28985351 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.28985351 0.         0.         0.28985351 0.19411827 0.\n",
            "  0.         0.28985351 0.         0.         0.13811675 0.\n",
            "  0.         0.28985351 0.         0.         0.         0.28985351\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.28985351 0.28985351 0.28985351 0.         0.         0.\n",
            "  0.         0.28985351 0.         0.         0.28985351]\n",
            " [0.24252094 0.24252094 0.24252094 0.11556253 0.24252094 0.\n",
            "  0.         0.         0.         0.24252094 0.         0.\n",
            "  0.         0.         0.         0.24252094 0.         0.\n",
            "  0.         0.         0.24252094 0.         0.         0.24252094\n",
            "  0.         0.24252094 0.         0.         0.         0.24252094\n",
            "  0.         0.         0.         0.         0.1624191  0.\n",
            "  0.         0.         0.         0.         0.11556253 0.19566437\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24252094 0.48504189 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.1624191  0.         0.         0.         0.        ]]\n",
            "Sentiment Scores:\n",
            "{'neg': 0.0, 'neu': 0.894, 'pos': 0.106, 'compound': 0.8176}\n",
            "\n"
          ]
        }
      ]
    }
  ]
}